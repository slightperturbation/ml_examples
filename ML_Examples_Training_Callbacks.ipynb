{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Examples: Training Callbacks",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQxx5eUNPnpbyBxVglVRU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slightperturbation/ml_examples/blob/master/ML_Examples_Training_Callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-gHLf7Habt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Training Callbacks\n",
        "\n",
        "This builds on the MNIST Fashion Example to explore using callbacks during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTLUbN1aEEy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB4BgcmgFcRf",
        "colab_type": "text"
      },
      "source": [
        "# Input\n",
        "\n",
        "Input during training from in-memory numpy arrays drawn from the Tensorflow example dataset fashion_mnist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL-eJMnYFqa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(image_train, label_train), (image_test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel data from 8-bit integer representation [0, 255] to the floating point range [0, 1].\n",
        "image_train = image_train / 255.0\n",
        "image_test = image_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEGDU8RoFuu4",
        "colab_type": "text"
      },
      "source": [
        "# Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWsw0G6ZFydu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clefAt-BY5XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t4_wC_mGO4m",
        "colab_type": "text"
      },
      "source": [
        "# Early Stopping\n",
        "\n",
        "Callbacks are added to the model.fit() call here, but can also be added to model.evaluate() and model.predict().\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5QWjmp4GSWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "4d337f2a-f572-4224-cb58-73f0f58a0ce4"
      },
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    ''' Stop when reaching some desired metric.'''\n",
        "    keys = list(logs.keys())\n",
        "    print('\\nEnd epoch {} of training; got log keys: {}\\n'.format(epoch, keys))\n",
        "    if logs['accuracy'] > .89:\n",
        "      print('\\nStopping training!\\n')\n",
        "      self.model.stop_training = True\n",
        "callback = CustomCallback()\n",
        "\n",
        "model.fit(image_train, label_train, epochs=5, callbacks=[callback])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1862/1875 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.8306\n",
            "End epoch 0 of training; got log keys: ['loss', 'accuracy']\n",
            "\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4758 - accuracy: 0.8307\n",
            "Epoch 2/5\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8659\n",
            "End epoch 1 of training; got log keys: ['loss', 'accuracy']\n",
            "\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3621 - accuracy: 0.8659\n",
            "Epoch 3/5\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8819\n",
            "End epoch 2 of training; got log keys: ['loss', 'accuracy']\n",
            "\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3227 - accuracy: 0.8819\n",
            "Epoch 4/5\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8887\n",
            "End epoch 3 of training; got log keys: ['loss', 'accuracy']\n",
            "\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3007 - accuracy: 0.8888\n",
            "Epoch 5/5\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.8941\n",
            "End epoch 4 of training; got log keys: ['loss', 'accuracy']\n",
            "\n",
            "\n",
            "Stopping training!\n",
            "\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2816 - accuracy: 0.8941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e2aaf05c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6hxXjywa1dr",
        "colab_type": "text"
      },
      "source": [
        "# Save Model at Minimum Loss\n",
        "\n",
        "See [Keras docs example](https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/writing_your_own_callbacks.ipynb#scrollTo=_KEUtZgeYOLA) for more.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-m5699a0uR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a490d88c-f730-41c2-f04e-4afbbef3d812"
      },
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=0):\n",
        "        super(CustomCallback, self).__init__()\n",
        "        self.patience = patience\n",
        "        # best_weights to store the weights at which the minimum loss occurs.\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize the best as infinity.\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get(\"loss\")\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best weights if current results is better (less).\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
        "\n",
        "callback = CustomCallback()\n",
        "\n",
        "model.fit(image_train, label_train, epochs=100, callbacks=[callback])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2657 - accuracy: 0.9005\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2535 - accuracy: 0.9053\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2417 - accuracy: 0.9109\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2313 - accuracy: 0.9119\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2221 - accuracy: 0.9165\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2148 - accuracy: 0.9186\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2080 - accuracy: 0.9227\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1983 - accuracy: 0.9244\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1908 - accuracy: 0.9271\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1841 - accuracy: 0.9306\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1789 - accuracy: 0.9330\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1730 - accuracy: 0.9352\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1681 - accuracy: 0.9365\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1628 - accuracy: 0.9384\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1578 - accuracy: 0.9403\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1529 - accuracy: 0.9417\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1491 - accuracy: 0.9441\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1441 - accuracy: 0.9450\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1395 - accuracy: 0.9463\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1380 - accuracy: 0.9473\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1320 - accuracy: 0.9505\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1308 - accuracy: 0.9503\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1259 - accuracy: 0.9524\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1245 - accuracy: 0.9534\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1193 - accuracy: 0.9556\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1174 - accuracy: 0.9556\n",
            "Epoch 27/100\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9561Restoring model weights from the end of the best epoch.\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1179 - accuracy: 0.9561\n",
            "Epoch 00027: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e308acba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ByKI22dcEmg",
        "colab_type": "text"
      },
      "source": [
        "# Learning Rate Schedule\n",
        "\n",
        "Although it adds a big space for hparams to tune, it can be useful to adjust the learning rate (generally starting large and dialing it down) as training progresses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1_IdmSJcgYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5605738-dfac-4943-df39-af6d80ad4980"
      },
      "source": [
        "\n",
        "LR_SCHEDULE = [\n",
        "    # (epoch to start, learning rate) tuples\n",
        "    (3, 0.05),\n",
        "    (6, 0.01),\n",
        "    (9, 0.005),\n",
        "    (12, 0.001),\n",
        "]\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
        "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
        "        return lr\n",
        "    for i in range(len(LR_SCHEDULE)):\n",
        "        if epoch == LR_SCHEDULE[i][0]:\n",
        "            return LR_SCHEDULE[i][1]\n",
        "    return lr\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "model.fit(image_train, label_train, epochs=15, callbacks=[callback])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00000: Learning rate is 0.0010.\n",
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1135 - accuracy: 0.9566\n",
            "\n",
            "Epoch 00001: Learning rate is 0.0010.\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1122 - accuracy: 0.9571\n",
            "\n",
            "Epoch 00002: Learning rate is 0.0010.\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1108 - accuracy: 0.9574\n",
            "\n",
            "Epoch 00003: Learning rate is 0.0500.\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 3.4346 - accuracy: 0.4656\n",
            "\n",
            "Epoch 00004: Learning rate is 0.0500.\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.1298 - accuracy: 0.5371\n",
            "\n",
            "Epoch 00005: Learning rate is 0.0500.\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.0973 - accuracy: 0.5645\n",
            "\n",
            "Epoch 00006: Learning rate is 0.0100.\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.9012 - accuracy: 0.6174\n",
            "\n",
            "Epoch 00007: Learning rate is 0.0100.\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8790 - accuracy: 0.6231\n",
            "\n",
            "Epoch 00008: Learning rate is 0.0100.\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.8106 - accuracy: 0.6748\n",
            "\n",
            "Epoch 00009: Learning rate is 0.0050.\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.6635 - accuracy: 0.7491\n",
            "\n",
            "Epoch 00010: Learning rate is 0.0050.\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.6361 - accuracy: 0.7606\n",
            "\n",
            "Epoch 00011: Learning rate is 0.0050.\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.6209 - accuracy: 0.7686\n",
            "\n",
            "Epoch 00012: Learning rate is 0.0010.\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5910 - accuracy: 0.7820\n",
            "\n",
            "Epoch 00013: Learning rate is 0.0010.\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5876 - accuracy: 0.7823\n",
            "\n",
            "Epoch 00014: Learning rate is 0.0010.\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5846 - accuracy: 0.7841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e284a37b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}